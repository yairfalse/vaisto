; Vaisto Self-Hosted Lexer
; Transforms source text into a stream of tokens
;
; The lexer maintains position state (line, col) and uses Std.Regex
; for pattern matching. It's designed to produce helpful error messages
; with accurate source locations.

(ns Vaisto.Lexer)

(import Std.Regex)
(import Std.State)
(import Std.String)
(import Std.File)
(import Vaisto.Lexer.Types)

; Erlang/BEAM primitives
(extern erlang/byte_size [:binary] :int)
(extern erlang/binary_to_integer [:binary] :int)
(extern erlang/binary_to_float [:binary] :float)
(extern erlang/error [:any] :any)
(extern erlang/hd [(List :any)] :any)
(extern erlang/element [:int :any] :any)
(extern lists/reverse [(List :any)] (List :any))
(extern lists/append [(List :any) (List :any)] (List :any))

; --- Lexer State ---
; We use the process dictionary to track position during lexing.
; Keys: :lex-line, :lex-col, :lex-file

(defn init-state! [filename]
  (Std.State/set! :lex-file filename)
  (Std.State/set! :lex-line 1)
  (Std.State/set! :lex-col 1))

(defn current-loc []
  (Vaisto.Lexer.Types/loc
    (Std.State/get-or :lex-file "<unknown>")
    (Std.State/get-or :lex-line 1)
    (Std.State/get-or :lex-col 1)))

(defn advance-pos! [text]
  ; Update line/col based on consumed text
  (let [lines (Std.Regex/count-newlines text)
        len (erlang/byte_size text)]
    (if (> lines 0)
      (do
        (Std.State/update! :lex-line (fn [l] (+ l lines)))
        (Std.State/set! :lex-col (+ 1 (Std.Regex/last-line-length text))))
      (Std.State/update! :lex-col (fn [c] (+ c len))))))

; --- Token Pattern Type ---
(deftype TokenPattern [type :any pattern :string])

; --- Lexer Result Type ---
(deftype LexResult
  (Skip rest:string)
  (Got token:any rest:string))

; --- Token Patterns ---
(def token-patterns
  (list
    ; Whitespace (skip, but track position)
    (TokenPattern :whitespace "^[ \\t\\r]+")

    ; Newlines (track separately for line counting)
    (TokenPattern :newline "^\\n")

    ; Comments
    (TokenPattern :comment "^;[^\\n]*")

    ; String literals: "..." with escape sequences like \"
    ; Pattern: match opening ", then capture content (non-" non-\ chars OR escape sequences), then closing "
    (TokenPattern :string "^\"((?:[^\"\\\\]|\\\\.)*)\"")

    ; Character literals: \a \newline \space
    (TokenPattern :char-named "^\\\\(newline|space|tab|return)")
    (TokenPattern :char-simple "^\\\\(.)")

    ; Numbers (integer and float)
    (TokenPattern :float "^-?[0-9]+\\.[0-9]+")
    (TokenPattern :integer "^-?[0-9]+")

    ; Keywords :foo or operator keywords :. :-> etc
    (TokenPattern :keyword "^:([a-zA-Z_+\\-*/<>=!?.][a-zA-Z0-9_+\\-*/<>=!?.]*)")

    ; Delimiters
    (TokenPattern :lparen "^\\(")
    (TokenPattern :rparen "^\\)")
    (TokenPattern :lbracket "^\\[")
    (TokenPattern :rbracket "^\\]")
    (TokenPattern :lbrace "^\\{")
    (TokenPattern :rbrace "^\\}")

    ; Pipe (for cons patterns like [h | t])
    (TokenPattern :pipe "^\\|")

    ; Symbols (identifiers and operators)
    ; Must come after keywords and numbers
    (TokenPattern :symbol "^[a-zA-Z_+\\-*/<>=!?][a-zA-Z0-9_+\\-*/<>=!?.]*")

    ; Dot accessor (for row polymorphism)
    (TokenPattern :dot "^\\.")))

; --- Character literal mapping ---
; Using cond and char literals!

(defn char-name-to-code [name]
  (cond
    [(== name "newline") \newline]
    [(== name "space") \space]
    [(== name "tab") \tab]
    [(== name "return") \return]
    [:else (erlang/error {:unknown-char-name name})]))

; --- Symbol to keyword mapping ---

(defn symbol-to-keyword-token [loc name]
  (cond
    [(== name "def") (Vaisto.Lexer.Types/KwDef loc)]
    [(== name "defn") (Vaisto.Lexer.Types/KwDefn loc)]
    [(== name "deftype") (Vaisto.Lexer.Types/KwDeftype loc)]
    [(== name "defrecord") (Vaisto.Lexer.Types/KwDefrecord loc)]
    [(== name "let") (Vaisto.Lexer.Types/KwLet loc)]
    [(== name "if") (Vaisto.Lexer.Types/KwIf loc)]
    [(== name "cond") (Vaisto.Lexer.Types/KwCond loc)]  ; Added cond
    [(== name "match") (Vaisto.Lexer.Types/KwMatch loc)]
    [(== name "fn") (Vaisto.Lexer.Types/KwFn loc)]
    [(== name "do") (Vaisto.Lexer.Types/KwDo loc)]
    [(== name "ns") (Vaisto.Lexer.Types/KwNs loc)]
    [(== name "import") (Vaisto.Lexer.Types/KwImport loc)]
    [(== name "extern") (Vaisto.Lexer.Types/KwExtern loc)]
    [:else (Vaisto.Lexer.Types/Symbol loc name)]))

; --- Core Lexer Logic ---

(defn make-token [match-result]
  ; match-result has :type :full :groups :rest
  (let [loc (current-loc)
        typ (. match-result :type)
        full (. match-result :full)
        groups (. match-result :groups)
        rest (. match-result :rest)]
    ; Advance position for consumed text
    (advance-pos! full)
    ; Create appropriate token - returns LexResult sum type
    (cond
      ; Skip whitespace/newline/comment
      [(== typ :whitespace) (Skip rest)]
      [(== typ :newline) (Skip rest)]
      [(== typ :comment) (Skip rest)]

      ; Delimiters
      [(== typ :lparen) (Got (Vaisto.Lexer.Types/LParen loc) rest)]
      [(== typ :rparen) (Got (Vaisto.Lexer.Types/RParen loc) rest)]
      [(== typ :lbracket) (Got (Vaisto.Lexer.Types/LBracket loc) rest)]
      [(== typ :rbracket) (Got (Vaisto.Lexer.Types/RBracket loc) rest)]
      [(== typ :lbrace) (Got (Vaisto.Lexer.Types/LBrace loc) rest)]
      [(== typ :rbrace) (Got (Vaisto.Lexer.Types/RBrace loc) rest)]

      ; Pipe symbol (for cons patterns)
      [(== typ :pipe) (Got (Vaisto.Lexer.Types/Symbol loc "|") rest)]

      ; Literals
      [(== typ :integer)
       (Got (Vaisto.Lexer.Types/IntLit loc (erlang/binary_to_integer full)) rest)]
      [(== typ :float)
       (Got (Vaisto.Lexer.Types/FloatLit loc (erlang/binary_to_float full)) rest)]
      [(== typ :string)
       (Got (Vaisto.Lexer.Types/StringLit loc (erlang/hd groups)) rest)]

      ; Character literals
      [(== typ :char-named)
       (Got (Vaisto.Lexer.Types/CharLit loc (char-name-to-code (erlang/hd groups))) rest)]
      [(== typ :char-simple)
       (Got (Vaisto.Lexer.Types/CharLit loc (Std.String/first-char (erlang/hd groups))) rest)]

      ; Keywords and symbols
      [(== typ :keyword)
       (Got (Vaisto.Lexer.Types/Keyword loc (erlang/hd groups)) rest)]
      [(== typ :symbol)
       (Got (symbol-to-keyword-token loc full) rest)]
      [(== typ :dot)
       (Got (Vaisto.Lexer.Types/Symbol loc ".") rest)]

      [:else (erlang/error {:unknown-token-type typ})])))

; Try to match next token from input
; Returns LexResult: either (Skip rest) or (Got token rest)
(defn next-token [input]
  (if (== (erlang/byte_size input) 0)
    (Got (Vaisto.Lexer.Types/EOF (current-loc)) "")
    (match (try-patterns input token-patterns)
      [(Some result) (make-token result)]
      [(None) (erlang/error {:unexpected-char
                             :char (Std.Regex/peek input 1)
                             :loc (current-loc)})])))

; Try patterns in order until one matches
(defn try-patterns [input patterns]
  (match patterns
    [[] (None)]
    [[pat | rest]
      (let [t (erlang/element 2 pat)   ; :type field is at index 2
            p (erlang/element 3 pat)]  ; :pattern field is at index 3
        (match (Std.Regex/match-groups input p)
          [(Some m) (Some #{ :type t
                             :full (. m :full)
                             :groups (. m :groups)
                             :rest (. m :rest) })]
          [(None) (try-patterns input rest)]))]))

; --- Main Entry Point ---

; Tokenize a string, returning a list of tokens
(defn tokenize [source filename]
  (Std.State/with-isolated-state
    (fn []
      (init-state! filename)
      (tokenize-loop source (list)))))

; Internal tokenization loop
; Pattern matches on LexResult sum type
(defn tokenize-loop [input acc]
  (match (next-token input)
    ; Skipped whitespace/comment, continue with rest
    [(Skip rest-input)
      (tokenize-loop rest-input acc)]
    ; Got a token - check if EOF or continue
    [(Got tok rest-input)
      (match tok
        [{:EOF _}
          ; Done - reverse accumulated tokens and add EOF
          (lists/reverse (cons tok acc))]
        [_
          ; Continue with this token
          (tokenize-loop rest-input (cons tok acc))])]))

; Convenience: tokenize from a file
(defn tokenize-file [path]
  (match (Std.File/read-file path)
    [(Ok content) (Ok (tokenize content path))]
    [(Err e) (Err e)]))
